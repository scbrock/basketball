{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(action='once')\n",
    "\n",
    "cwd = os.getcwd()\n",
    "print(cwd)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: create dictionaries and then access them throughout the program\n",
    "\n",
    "team_list = ['GSW', 'HOU', 'OKC', 'SAC', 'CLE', 'POR', 'NOP', 'TOR', 'IND', \n",
    "         'BOS', 'NYK', 'LAC', 'SAS', 'CHI', 'CHA', 'MIN', 'BKN', 'PHX', \n",
    "         'WAS', 'UTA', 'DEN', 'MIA', 'DET', 'DAL', 'ORL', 'MIL', 'LAL', \n",
    "         'PHI', 'ATL', 'MEM']\n",
    "\n",
    "file = 'players/nba_player_1516.csv'\n",
    "\n",
    "def build_player_dictionary(file):\n",
    "    \n",
    "    data = pd.read_csv(file, index_col=0)\n",
    "    \n",
    "    team_dict = {}\n",
    "    \n",
    "    for index, row in data.iterrows():\n",
    "        team = row['TEAM']\n",
    "        name = row['PLAYER']\n",
    "\n",
    "        row_as_dict = row.to_dict()\n",
    "\n",
    "        if team_dict.get(team) is None:\n",
    "            team_dict[team] = {}\n",
    "            team_dict[team][name] = row_as_dict\n",
    "        else:\n",
    "            team_dict[team][name] = row_as_dict\n",
    "    return team_dict\n",
    "\n",
    "# players_dict_1516 = build_player_dictionary(file)\n",
    "\n",
    "# print(players_dict_1516.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_team_dictionary(file):\n",
    "    data = pd.read_csv(file, index_col=0)\n",
    "    team_dict = {}\n",
    "    \n",
    "    for index, row in data.iterrows():\n",
    "        team = row['TEAM']\n",
    "        stats = row.to_dict()\n",
    "        \n",
    "        team_dict[team] = stats\n",
    "        \n",
    "    return team_dict\n",
    "\n",
    "# file = 'teams/nba_team_1011.csv'\n",
    "\n",
    "# mydict = build_team_dictionary(file)\n",
    "\n",
    "# print(mydict['Chicago Bulls'].keys())\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(players_dict_1516.get('HOU').get('James Harden').keys())\n",
    "\n",
    "cols_drop = ['PLAYER', 'TEAM']  # all other columns are relevant\n",
    "\n",
    "drop_list = ['PLAYER', 'TEAM', 'W', 'L', 'MIN', 'FGM', 'FGA', '3PM', '3PA', 'FTM', 'FTA', 'OREB', 'DREB', 'PF', 'FP', 'DD2', 'TD3']\n",
    "\n",
    "player_stats = ['PLAYER', 'TEAM', 'AGE', 'GP', 'W', 'L', 'MIN', 'PTS', 'FGM', 'FGA', 'FG%', '3PM', '3PA', '3P%', 'FTM', 'FTA', 'FT%', 'OREB', 'DREB', 'REB', 'AST', 'TOV', 'STL', 'BLK', 'PF', 'FP', 'DD2', 'TD3', '+/-']\n",
    "\n",
    "team_stats = ['TEAM', 'GP', 'W', 'L', 'WIN%', 'MIN', 'PTS', 'FGM', 'FGA', 'FG%', '3PM', '3PA', '3P%', 'FTM', 'FTA', 'FT%', 'OREB', 'DREB', 'REB', 'AST', 'TOV', 'STL', 'BLK', 'BLKA', 'PF', 'PFD', '+/-']\n",
    "\n",
    "team_drop = ['TEAM', 'GP', 'W', 'L', 'MIN', 'FGM', 'FGA', '3PM', '3PA', 'FTM', 'FTA', 'FT%', 'OREB', 'DREB', 'BLKA', 'PF', 'PFD']\n",
    "\n",
    "player_keep = [x for x in player_stats if x not in drop_list]\n",
    "team_keep = [x for x in team_stats if x not in team_drop]\n",
    "\n",
    "\n",
    "# create top players feature vector\n",
    "def top_players_vec(team):\n",
    "    team_df = pd.DataFrame.from_dict(team)  # column == player == feature vector (need to drop name/team) \n",
    "    # select top 8 players with most minutes:\n",
    "    # sort based on minutes played\n",
    "    team_df = team_df.sort_values('MIN', axis=1, ascending=False)\n",
    "    team_df = team_df.drop(['PLAYER', 'TEAM'])\n",
    "    team_df = team_df.iloc[:,0:8]\n",
    "    #print('player stats columns:', team_df.index.values)\n",
    "    team_vec = team_df.values\n",
    "    team_vec = np.array(team_vec).ravel()\n",
    "    return team_vec\n",
    "\n",
    "# create team stats feature vector\n",
    "def team_stats_vec(team):\n",
    "    #print(team)\n",
    "    #team_df = pd.DataFrame.from_dict(team)\n",
    "    team_df = pd.DataFrame(team, index=[0])\n",
    "    team_df = team_df.drop(team_drop, axis=1)\n",
    "    #print('team stats columns:', team_df.columns.values)\n",
    "    team_vec = team_df.values\n",
    "    return team_vec[0]\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# headers for player stats\n",
    "#['+/-' '3P%' '3PA' '3PM' 'AGE' 'AST' 'BLK' 'DD2' 'DREB' 'FG%' 'FGA' 'FGM'\n",
    "# 'FP' 'FT%' 'FTA' 'FTM' 'GP' 'L' 'MIN' 'OREB' 'PF' 'PTS' 'REB' 'STL' 'TD3'\n",
    "# 'TOV' 'W']\n",
    "\n",
    "# headers for team stats\n",
    "# ['+/-' '3P%' 'AST' 'BLK' 'FG%' 'PTS' 'REB' 'STL' 'TOV' 'WIN%']\n",
    "team_headers = ['+/-', '3P%', 'AST', 'BLK', 'FG%', 'PTS', 'REB', 'STL', 'TOV', 'WIN%']\n",
    "player_headers = ['+/-', '3P%', '3PA', '3PM', 'AGE', 'AST', 'BLK', 'DD2', 'DREB', 'FG%', 'FGA', 'FGM',\n",
    "                  'FP', 'FT%', 'FTA', 'FTM', 'GP', 'L', 'MIN', 'OREB', 'PF', 'PTS', 'REB', 'STL', 'TD3',\n",
    "                  'TOV', 'W']\n",
    "# Long NBA team name to abbreviations dictionary\n",
    "team_name_dict = {}\n",
    "team_name_dict['Houston Rockets'] = 'HOU'\n",
    "team_name_dict['Toronto Raptors'] = 'TOR'\n",
    "team_name_dict['Golden State Warriors'] = 'GSW'\n",
    "team_name_dict['Boston Celtics'] = 'BOS'\n",
    "team_name_dict['Philadelphia 76ers'] = 'PHI'\n",
    "team_name_dict['Cleveland Cavaliers'] = 'CLE'\n",
    "team_name_dict['Portland Trail Blazers'] = 'POR'\n",
    "team_name_dict['Indiana Pacers'] = 'IND'\n",
    "team_name_dict['Oklahoma City Thunder'] = 'OKC'\n",
    "team_name_dict['New Orleans Pelicans'] = 'NOP'\n",
    "team_name_dict['Utah Jazz'] = 'UTA'\n",
    "team_name_dict['San Antonio Spurs'] = 'SAS'\n",
    "team_name_dict['Minnesota Timberwolves'] = 'MIN'\n",
    "team_name_dict['Denver Nuggets'] = 'DEN'\n",
    "team_name_dict['Miami Heat'] = 'MIA'\n",
    "team_name_dict['Milwaukee Bucks'] = 'MIL'\n",
    "team_name_dict['Washington Wizards'] = 'WAS'\n",
    "team_name_dict['Los Angeles Clippers'] = 'LAC'\n",
    "team_name_dict['Detroit Pistons'] = 'DET'\n",
    "team_name_dict['Charlotte Hornets'] = 'CHA'\n",
    "team_name_dict['Los Angeles Lakers'] = 'LAL'\n",
    "team_name_dict['New York Knicks'] = 'NYK'\n",
    "team_name_dict['Brooklyn Nets'] = 'BKN'\n",
    "team_name_dict['Sacramento Kings'] = 'SAC'\n",
    "team_name_dict['Chicago Bulls'] = 'CHI'\n",
    "team_name_dict['Orlando Magic'] = 'ORL'\n",
    "team_name_dict['Dallas Mavericks'] = 'DAL'\n",
    "team_name_dict['Atlanta Hawks'] = 'ATL'\n",
    "team_name_dict['Memphis Grizzlies'] = 'MEM'\n",
    "team_name_dict['Phoenix Suns'] = 'PHX'\n",
    "team_name_dict['Vancouver Grizzlies'] = 'VAN'\n",
    "team_name_dict['Seattle SuperSonics'] = 'SEA'\n",
    "team_name_dict['New Jersey Nets'] = 'NJN'\n",
    "team_name_dict['Charlotte Hornets'] = 'CHH' # last season was 2001-02 -> new orleans hornets\n",
    "team_name_dict['Charlotte Hornets 2'] = 'CHA' # 1415 seaons onwards need to edit csvs\n",
    "team_name_dict['Charlotte Bobcats'] = 'CHB' # last season was 2013-14 -> charlotte hornets (changed back)\n",
    "team_name_dict['New Orleans Hornets'] = 'NOH' # last season was 2012-2013 COULD ALSO BE NOK!!!\n",
    "\n",
    "name_reverse_dictionary = {}\n",
    "for key, value in team_name_dict.items():\n",
    "    name_reverse_dictionary[value] = key\n",
    "    \n",
    "name_reverse_dictionary['NOK'] = 'New Orleans Hornets'\n",
    "name_reverse_dictionary['NOH'] = 'New Orleans Hornets'\n",
    "name_reverse_dictionary['CHA'] = 'Charlotte Hornets'\n",
    "\n",
    "# prepare data\n",
    "\n",
    "cwd = os.getcwd()\n",
    "\n",
    "team_folder = cwd+'/teams/'\n",
    "player_folder = cwd+'/players/'\n",
    "\n",
    "teams_dict = {}\n",
    "players_dict = {}\n",
    "\n",
    "def get_next_season(season):\n",
    "    beginning = int(season[:2])\n",
    "    year = None\n",
    "    if beginning < 20:\n",
    "        year = beginning + 2000\n",
    "    else:\n",
    "        year = beginning + 1900\n",
    "    return str(year+1)[2:] + str(year+2)[2:]\n",
    "\n",
    "team_list = ['Houston Rockets', 'Toronto Raptors', 'Golden State Warriors', 'Boston Celtics', 'Philadelphia 76ers', \n",
    "             'Cleveland Cavaliers', 'Portland Trail Blazers', 'Indiana Pacers', 'Oklahoma City Thunder', \n",
    "             'New Orleans Pelicans', 'Utah Jazz', 'San Antonio Spurs', 'Minnesota Timberwolves', 'Denver Nuggets', \n",
    "             'Miami Heat', 'Milwaukee Bucks', 'Washington Wizards', 'Los Angeles Clippers', 'Detroit Pistons', 'Charlotte Hornets',\n",
    "             'Los Angeles Lakers', 'New York Knicks', 'Brooklyn Nets', 'Sacramento Kings', 'Chicago Bulls', \n",
    "             'Orlando Magic', 'Dallas Mavericks', 'Atlanta Hawks', 'Memphis Grizzlies', 'Phoenix Suns']\n",
    "\n",
    "\n",
    "def get_short_name(team):\n",
    "    return team_name_dict.get(team)\n",
    "\n",
    "def get_long_name(team):\n",
    "    return name_reverse_dictionary.get(team)\n",
    "\n",
    "def name_conversion(team):\n",
    "    team_dict = {}\n",
    "    team_dict['Vancouver Grizzlies'] = 'Memphis Grizzlies'\n",
    "    team_dict['Charlotte Hornets'] = 'New Orleans Hornets'\n",
    "    team_dict['Seattle SuperSonics'] = 'Oklahoma City Thunder'\n",
    "    team_dict['New Jersey Nets'] = 'Brooklyn Nets'\n",
    "    team_dict['New Orleans Hornets'] = 'New Orleans Pelicans'\n",
    "    team_dict['Charlotte Bobcats'] = 'Charlotte Hornets'\n",
    "    return team_dict.get(team)\n",
    "\n",
    "def next_season_team(team_long, season):\n",
    "    new_team = team_long\n",
    "    \n",
    "    if season == '0001' and team_long == 'Vancouver Grizzlies':\n",
    "        new_team = name_conversion(team_long)\n",
    "    elif season == '0102' and team_long == 'Charlotte Hornets':\n",
    "        new_team = name_conversion(team_long)\n",
    "    elif season == '0708' and team_long == 'Seattle SuperSonics':\n",
    "        new_team = name_conversion(team_long)\n",
    "    elif season == '1112' and team_long == 'New Jersey Nets':\n",
    "        new_team = name_conversion(team_long)\n",
    "    elif season == '1314' and team_long == 'Charlotte Bobcats':\n",
    "        new_team = name_conversion(team_long)\n",
    "    elif season == '1213' and team_long =='New Orleans Hornets':\n",
    "        new_team = name_conversion(team_long)\n",
    "    \n",
    "    return new_team\n",
    "\n",
    "def build_dictionaries():\n",
    "    print('Building dictionaries...')\n",
    "    \n",
    "    for file in os.listdir(team_folder):\n",
    "        season = str(file[-8:-4])\n",
    "        season_teams = build_team_dictionary(team_folder+file)\n",
    "        #print(season_teams.keys())\n",
    "        teams_dict[season] = season_teams\n",
    "\n",
    "    for file in os.listdir(player_folder):\n",
    "        season = str(file[-8:-4])\n",
    "        season_players = build_player_dictionary(player_folder+file)\n",
    "        player_dict = {}\n",
    "        players_dict[season] = season_players\n",
    "        \n",
    "    print('Done building dictionaries.')\n",
    "\n",
    "    print(players_dict.keys())\n",
    "    print(teams_dict.keys())\n",
    "\n",
    "build_dictionaries()\n",
    "    \n",
    "\n",
    "num_seasons = len(players_dict)\n",
    "\n",
    "\n",
    "highest_season = '1718'\n",
    "print(num_seasons)\n",
    "\n",
    "def feature_gen():\n",
    "    print('Generating features...')\n",
    "    i = 0\n",
    "    if len(teams_dict) == 0:\n",
    "        build_dictionaries()\n",
    "    \n",
    "    x_vec_length = None\n",
    "\n",
    "    x_matrix = pd.DataFrame({})\n",
    "    y_vec = []\n",
    "    \n",
    "    for season, t_dict in players_dict.items():\n",
    "        i += 1\n",
    "        #print(season)\n",
    "        #print(get_next_season(season))\n",
    "        if i >= num_seasons-1 or season == highest_season:\n",
    "            continue\n",
    "\n",
    "        next_season = get_next_season(season)\n",
    "\n",
    "        # for each season, generate feature vectors for each team\n",
    "        for t, t_stats in t_dict.items():\n",
    "            plyr_vec = top_players_vec(t_stats)\n",
    "\n",
    "            next_season_tname = next_season_team(get_long_name(t), season)\n",
    "            if get_long_name(t) == 'LA Clippers':\n",
    "                print(season)\n",
    "                print('LA Clippers')\n",
    "            try:\n",
    "                team_vec = team_stats_vec(teams_dict[season][get_long_name(t)])\n",
    "            except Exception as e:\n",
    "                print('Ignoring case')\n",
    "                print('team', t)\n",
    "                print('error', e)\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                next_team_vec = team_stats_vec(teams_dict[next_season][next_season_tname])\n",
    "            except Exception as e:\n",
    "                print('Ignoring case')\n",
    "                print(e)\n",
    "                continue\n",
    "            # create performance categories for win percentage\n",
    "            next_s_t_winp = next_team_vec[-1]  # index for the win%\n",
    "            if next_s_t_winp >= 0.8:\n",
    "                next_s_t_winp = 4\n",
    "            elif next_s_t_winp >= 0.6:\n",
    "                next_s_t_winp = 3\n",
    "            elif next_s_t_winp >= 0.4:\n",
    "                next_s_t_winp = 2\n",
    "            elif next_s_t_winp >= 0.2:\n",
    "                next_s_t_winp = 1\n",
    "            else:\n",
    "                next_s_t_winp = 0\n",
    "\n",
    "#             print('plyr_vec', plyr_vec)\n",
    "#             print('team_vec', team_vec)\n",
    "#             print('next_team_vec', next_team_vec)\n",
    "\n",
    "            # Create headers\n",
    "            player_h_portion = [[x+'_1', x+'_2', x+'_3', x+'_4', x+'_5', x+'_6', x+'_7', x+'_8'] for x in player_headers]\n",
    "            player_labels = []\n",
    "            for l in player_h_portion:\n",
    "                player_labels += l\n",
    "\n",
    "            curr_headers = team_headers+player_labels\n",
    "\n",
    "            x_vec = np.concatenate((team_vec, plyr_vec))\n",
    "            #print(x_vec)\n",
    "            x_series = pd.DataFrame(np.reshape(x_vec, (1,len(x_vec))), columns = curr_headers)\n",
    "            #print(x_series)\n",
    "            x_vec_label = next_s_t_winp  # set label\n",
    "\n",
    "            #x_matrix.extend(x_series)\n",
    "            x_matrix = pd.concat([x_matrix, x_series], ignore_index=True)\n",
    "            y_vec.append(x_vec_label)\n",
    "\n",
    "            x_vec_length = len(x_vec)  # this needs to only be executed once\n",
    "\n",
    "    xdf = pd.DataFrame(x_matrix)\n",
    "    print(xdf)\n",
    "    ydf = pd.DataFrame(y_vec)\n",
    "    print(len(x_matrix))\n",
    "    print(players_dict.keys())\n",
    "    print('Done generating features.')\n",
    "    \n",
    "    return xdf, ydf\n",
    "\n",
    "xdf, ydf = feature_gen()\n",
    "\n",
    "xdf.to_csv('x_matrix.csv', sep=',')\n",
    "ydf.to_csv('y_vec.csv', sep=',')\n",
    "\n",
    "#print(len(x_matrix), len(x_matrix[0]))\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: \n",
      " [[ 6.83552408e-02 -2.41290737e-02  5.48456891e-02  2.65795165e-02\n",
      "  -6.72215108e-02  7.99176414e-03 -3.52532382e-02 -8.57046372e-02\n",
      "   6.19405799e-02 -1.33991060e+00 -2.45736386e-03  1.80137956e-02\n",
      "  -1.31015204e-02 -5.62464426e-03  8.38440796e-03  1.30745236e-02\n",
      "  -1.09206864e-02 -2.43128854e-02 -6.21950409e-03 -3.66041823e-03\n",
      "  -1.93633378e-04 -2.50687910e-03 -7.63254992e-03  1.59521420e-04\n",
      "  -2.32710468e-03 -2.27057596e-04 -2.06790875e-01 -3.84723842e-02\n",
      "  -4.83949066e-02 -2.94128207e-02 -2.29594294e-02 -5.96781628e-02\n",
      "  -5.95417474e-02 -6.51729123e-02 -3.06338124e-01 -1.57283079e-01\n",
      "   2.47169702e-01 -3.06439517e-01 -8.60992301e-01 -2.72904286e-01\n",
      "   1.65994684e+00  9.97398406e-01 -1.28771977e-02 -3.56153540e-02\n",
      "  -1.27949866e-02  1.28634305e-03 -7.60227431e-04  3.38440428e-03\n",
      "  -9.44189834e-04 -5.46468259e-03 -3.39936295e-01 -8.35845097e-01\n",
      "   3.00898964e-01 -2.58155397e-01 -2.96725458e-01 -5.63592632e-01\n",
      "  -4.25306763e-02 -1.92427798e-01 -6.59508105e-01 -1.75680582e+00\n",
      "   6.11652772e-01 -5.72555480e-01 -5.91432296e-01 -7.26866053e-01\n",
      "   1.01820929e-01 -2.36941191e-01 -3.32470777e-03 -1.83039182e-02\n",
      "  -1.30111726e-03  1.07426455e-02 -1.96014287e-03  9.87468207e-03\n",
      "   2.24457142e-02 -3.31822898e-02  1.36870569e+00  9.16066361e-02\n",
      "   1.04694602e-01  2.90667463e-01  4.74400542e-01 -3.32457430e-01\n",
      "  -8.69749115e-01 -6.05383271e-01 -1.55886716e-03 -8.40941547e-02\n",
      "  -1.18593830e-02 -9.60250003e-03 -8.61446402e-03  6.26922028e-03\n",
      "   2.09624537e-02 -1.80584243e-03  3.24994227e-02 -2.81755565e-01\n",
      "  -7.19823071e-02 -7.16132117e-02  6.56910409e-03 -1.03880488e-01\n",
      "   1.94528601e-01 -5.77083896e-02 -1.71420263e+00  2.15504757e-01\n",
      "   5.09251470e-01 -5.96172293e-01 -1.86090099e+00 -2.50804801e-01\n",
      "   2.31712606e+00  1.41832777e+00  2.59204450e-01  6.01748636e-01\n",
      "  -1.92108267e-01  1.54572406e-01  1.64872210e-01  3.04423973e-01\n",
      "   4.98113850e-02  1.29302193e-01  5.34688994e-03 -5.92257923e-03\n",
      "   1.20053520e-02 -2.14027221e-03 -2.71391940e-03 -6.90526204e-04\n",
      "  -7.22823386e-03  1.70071207e-03  8.91055338e-02  3.42567824e-02\n",
      "   2.20066301e-01 -7.65532379e-03 -3.04748684e-01 -1.75731181e-01\n",
      "  -4.98788141e-01  1.26609202e-01 -8.67812015e-01 -2.12491213e-01\n",
      "  -8.03531821e-02 -2.83331271e-01 -4.24026372e-01 -3.34329934e-02\n",
      "   2.05224165e+00  5.56035930e-01 -3.24431644e-03  2.99147711e-03\n",
      "  -1.02053669e-03  1.74403993e-03 -2.09046862e-03 -1.88099698e-03\n",
      "  -3.88742619e-05  2.24263395e-03  4.95128263e-03 -1.11429989e-02\n",
      "  -5.82242036e-03  2.31291913e-03 -7.03574637e-03 -1.24674185e-03\n",
      "  -2.42663200e-03 -1.14903994e-03  3.33264591e-02 -2.35389566e-02\n",
      "   8.43805609e-03  2.14967885e-02 -9.78436709e-03  4.18088373e-04\n",
      "   1.45073650e-02 -1.52038186e-02  1.09483306e+00 -6.52955659e-02\n",
      "   1.40038114e-01  1.01508635e-01  5.26890662e-01 -4.77928601e-01\n",
      "  -5.22623438e-01 -7.33983157e-01  8.12346393e-02 -9.50948568e-02\n",
      "  -2.51607386e-02  4.62448645e-03 -8.63439822e-02  5.88440829e-02\n",
      "  -3.92903571e-02 -1.20447414e-01  5.59476983e-01 -3.75059988e-01\n",
      "   2.76746715e-02  2.29278402e-01  7.28552897e-01 -6.05886839e-02\n",
      "  -1.42440995e+00 -7.74276795e-01 -1.60248792e+00 -6.63091182e-01\n",
      "   1.39454178e-01 -4.56065377e-01 -6.75537914e-01 -1.45815774e-01\n",
      "   6.80339373e-01  5.41333520e-01 -9.09134397e-01 -1.74298324e+00\n",
      "   5.73694885e-01 -3.27414733e-01 -3.93053238e-01 -6.40975188e-01\n",
      "  -1.25901057e-01 -1.18336498e-01  1.65638480e-02 -2.78717506e-02\n",
      "   1.45603457e-01 -3.17340549e-03 -5.86860392e-02  1.01626078e-01\n",
      "  -2.05278621e-01 -7.88548122e-01  6.50087894e-02  3.46989132e-01\n",
      "  -3.36925443e-01  1.61189329e-02  2.14481494e-01  4.31329345e-01\n",
      "   7.50985750e-02  8.01320731e-02 -8.19559907e-03  1.41344760e-02\n",
      "   4.80188367e-03 -5.68879199e-04  4.94527775e-03 -6.34255129e-04\n",
      "   2.38775774e-03  3.39167389e-03]]\n",
      "Mean squared error: 0.84\n",
      "Variance score: -0.13\n",
      "Accuracy: 0.3879310344827586\n"
     ]
    }
   ],
   "source": [
    "# Linear regression model\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "\n",
    "Xdf = pd.read_csv('x_matrix.csv', sep=',', dtype='float', index_col=0)\n",
    "ydf = pd.read_csv('y_vec.csv', sep=',', dtype='float', index_col=0)\n",
    "\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xdf, ydf, test_size=0.2, random_state=20)\n",
    "\n",
    "# Create linear regression object\n",
    "lm = linear_model.LinearRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "lm.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "nba_y_pred = lm.predict(X_test)\n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients: \\n', lm.coef_)\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(y_test, nba_y_pred))\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score: %.2f' % r2_score(y_test, nba_y_pred))\n",
    "rounded = np.rint(nba_y_pred)\n",
    "print('Accuracy:', metrics.accuracy_score(y_test, rounded))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "405 174 405 174\n",
      "Model fit time: 404.2231001853943\n",
      "Fit the xgboost model\n",
      "Test accuracy: 0.5114942528735632\n",
      "Train accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# XGBoost model\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "Xdf = pd.read_csv('x_matrix.csv', sep=',', dtype='float', index_col=0)\n",
    "Xdf = Xdf.values\n",
    "ydf = pd.read_csv('y_vec.csv', sep=',', dtype='float', index_col=0)\n",
    "ydf = np.array(ydf).ravel()\n",
    "\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xdf, ydf, test_size=0.3, random_state=20)\n",
    "\n",
    "grid = {'learning_rate': [0.1, 0.01, 0.001], 'n_estimators': [100,200,300],\n",
    "                  'max_depth': [10,20,30]}\n",
    "\n",
    "model = GridSearchCV(estimator=XGBClassifier(), param_grid=grid, cv=3)\n",
    "\n",
    "print(len(X_train), len(X_test), len(y_train), len(y_test))\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print('Model fit time:', end-start)\n",
    "\n",
    "print(\"Fit the xgboost model\")\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_train = model.predict(X_train)\n",
    "print('Test accuracy:', accuracy_score(y_test, y_pred))\n",
    "print('Train accuracy:', accuracy_score(y_train, y_pred_train))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.5114942528735632\n",
      "Train accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "print('Test accuracy:', accuracy_score(y_test, y_pred))\n",
    "print('Train accuracy:', accuracy_score(y_train, y_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.1, 'max_depth': 20, 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "params = model.best_params_\n",
    "print(params)\n",
    "# {'learning_rate': 0.1, 'max_depth': 20, 'n_estimators': 200}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model file\n",
    "import pickle\n",
    "pickle.dump(model, open(\"model.pickle.dat\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "405/405 [==============================] - 1s 2ms/step - loss: 1.6799 - acc: 0.4420\n",
      "Epoch 2/200\n",
      "405/405 [==============================] - 0s 784us/step - loss: 1.0821 - acc: 0.5531\n",
      "Epoch 3/200\n",
      "405/405 [==============================] - 0s 801us/step - loss: 1.1961 - acc: 0.4617\n",
      "Epoch 4/200\n",
      "405/405 [==============================] - 0s 787us/step - loss: 1.0749 - acc: 0.5309\n",
      "Epoch 5/200\n",
      "405/405 [==============================] - 0s 783us/step - loss: 1.0827 - acc: 0.4988\n",
      "Epoch 6/200\n",
      "405/405 [==============================] - 0s 829us/step - loss: 0.9826 - acc: 0.5481\n",
      "Epoch 7/200\n",
      "405/405 [==============================] - 0s 815us/step - loss: 1.0041 - acc: 0.5728\n",
      "Epoch 8/200\n",
      "405/405 [==============================] - 0s 781us/step - loss: 0.9853 - acc: 0.5630\n",
      "Epoch 9/200\n",
      "405/405 [==============================] - 0s 791us/step - loss: 0.9625 - acc: 0.6099\n",
      "Epoch 10/200\n",
      "405/405 [==============================] - 0s 786us/step - loss: 1.0032 - acc: 0.5457\n",
      "Epoch 11/200\n",
      "405/405 [==============================] - 0s 785us/step - loss: 1.0530 - acc: 0.5062\n",
      "Epoch 12/200\n",
      "405/405 [==============================] - 0s 804us/step - loss: 0.9519 - acc: 0.5827\n",
      "Epoch 13/200\n",
      "405/405 [==============================] - 0s 892us/step - loss: 0.9694 - acc: 0.5753\n",
      "Epoch 14/200\n",
      "405/405 [==============================] - 0s 967us/step - loss: 0.9395 - acc: 0.6123\n",
      "Epoch 15/200\n",
      "405/405 [==============================] - 0s 828us/step - loss: 1.0241 - acc: 0.5407\n",
      "Epoch 16/200\n",
      "405/405 [==============================] - 0s 853us/step - loss: 0.9124 - acc: 0.6173\n",
      "Epoch 17/200\n",
      "405/405 [==============================] - 0s 807us/step - loss: 0.9716 - acc: 0.5531\n",
      "Epoch 18/200\n",
      "405/405 [==============================] - 0s 800us/step - loss: 0.9732 - acc: 0.5778\n",
      "Epoch 19/200\n",
      "405/405 [==============================] - 0s 816us/step - loss: 0.8901 - acc: 0.6296\n",
      "Epoch 20/200\n",
      "405/405 [==============================] - 0s 861us/step - loss: 0.8871 - acc: 0.6222\n",
      "Epoch 21/200\n",
      "405/405 [==============================] - 0s 1ms/step - loss: 0.9564 - acc: 0.5728\n",
      "Epoch 22/200\n",
      "405/405 [==============================] - 0s 956us/step - loss: 0.9330 - acc: 0.5975\n",
      "Epoch 23/200\n",
      "405/405 [==============================] - 0s 886us/step - loss: 0.9257 - acc: 0.6074\n",
      "Epoch 24/200\n",
      "405/405 [==============================] - 0s 874us/step - loss: 0.8988 - acc: 0.5975\n",
      "Epoch 25/200\n",
      "405/405 [==============================] - 0s 854us/step - loss: 0.8903 - acc: 0.6469\n",
      "Epoch 26/200\n",
      "405/405 [==============================] - 0s 794us/step - loss: 0.9118 - acc: 0.6025\n",
      "Epoch 27/200\n",
      "405/405 [==============================] - 0s 889us/step - loss: 0.8565 - acc: 0.6494\n",
      "Epoch 28/200\n",
      "405/405 [==============================] - 0s 861us/step - loss: 0.9028 - acc: 0.6099\n",
      "Epoch 29/200\n",
      "405/405 [==============================] - 0s 832us/step - loss: 0.8629 - acc: 0.6420\n",
      "Epoch 30/200\n",
      "405/405 [==============================] - 0s 878us/step - loss: 0.8341 - acc: 0.6321\n",
      "Epoch 31/200\n",
      "405/405 [==============================] - 0s 828us/step - loss: 0.8538 - acc: 0.6370\n",
      "Epoch 32/200\n",
      "405/405 [==============================] - 0s 905us/step - loss: 0.8078 - acc: 0.6667\n",
      "Epoch 33/200\n",
      "405/405 [==============================] - 0s 880us/step - loss: 0.9032 - acc: 0.5975\n",
      "Epoch 34/200\n",
      "405/405 [==============================] - 0s 864us/step - loss: 0.8992 - acc: 0.6148\n",
      "Epoch 35/200\n",
      "405/405 [==============================] - 0s 850us/step - loss: 0.8149 - acc: 0.6469\n",
      "Epoch 36/200\n",
      "405/405 [==============================] - 0s 858us/step - loss: 0.8161 - acc: 0.6370\n",
      "Epoch 37/200\n",
      "405/405 [==============================] - 0s 844us/step - loss: 0.9295 - acc: 0.6198\n",
      "Epoch 38/200\n",
      "405/405 [==============================] - 0s 912us/step - loss: 0.8362 - acc: 0.6370\n",
      "Epoch 39/200\n",
      "405/405 [==============================] - 0s 906us/step - loss: 0.8053 - acc: 0.6543\n",
      "Epoch 40/200\n",
      "405/405 [==============================] - 0s 796us/step - loss: 0.9198 - acc: 0.5975\n",
      "Epoch 41/200\n",
      "405/405 [==============================] - 0s 1ms/step - loss: 0.8505 - acc: 0.6198\n",
      "Epoch 42/200\n",
      "405/405 [==============================] - 0s 1ms/step - loss: 0.7870 - acc: 0.6741\n",
      "Epoch 43/200\n",
      "405/405 [==============================] - 1s 2ms/step - loss: 0.8217 - acc: 0.6444\n",
      "Epoch 44/200\n",
      "405/405 [==============================] - 0s 832us/step - loss: 0.8834 - acc: 0.6370\n",
      "Epoch 45/200\n",
      "405/405 [==============================] - 0s 801us/step - loss: 0.8252 - acc: 0.6420\n",
      "Epoch 46/200\n",
      "405/405 [==============================] - 0s 888us/step - loss: 0.8151 - acc: 0.6642\n",
      "Epoch 47/200\n",
      "405/405 [==============================] - 0s 794us/step - loss: 0.8542 - acc: 0.6247\n",
      "Epoch 48/200\n",
      "405/405 [==============================] - 0s 778us/step - loss: 0.7706 - acc: 0.6691\n",
      "Epoch 49/200\n",
      "405/405 [==============================] - 0s 752us/step - loss: 0.8010 - acc: 0.6543\n",
      "Epoch 50/200\n",
      "405/405 [==============================] - 0s 740us/step - loss: 0.8156 - acc: 0.6593\n",
      "Epoch 51/200\n",
      "405/405 [==============================] - 0s 769us/step - loss: 0.7566 - acc: 0.6765\n",
      "Epoch 52/200\n",
      "405/405 [==============================] - 0s 866us/step - loss: 0.7255 - acc: 0.6815\n",
      "Epoch 53/200\n",
      "405/405 [==============================] - 0s 772us/step - loss: 0.7423 - acc: 0.6790\n",
      "Epoch 54/200\n",
      "405/405 [==============================] - 0s 805us/step - loss: 0.7563 - acc: 0.6988\n",
      "Epoch 55/200\n",
      "405/405 [==============================] - 0s 766us/step - loss: 0.8145 - acc: 0.6617\n",
      "Epoch 56/200\n",
      "405/405 [==============================] - 0s 794us/step - loss: 0.7419 - acc: 0.6840\n",
      "Epoch 57/200\n",
      "405/405 [==============================] - 0s 1ms/step - loss: 0.7079 - acc: 0.7062\n",
      "Epoch 58/200\n",
      "405/405 [==============================] - 0s 1ms/step - loss: 0.6883 - acc: 0.7012\n",
      "Epoch 59/200\n",
      "405/405 [==============================] - 1s 1ms/step - loss: 0.7496 - acc: 0.6790\n",
      "Epoch 60/200\n",
      "405/405 [==============================] - 0s 813us/step - loss: 0.7272 - acc: 0.6914\n",
      "Epoch 61/200\n",
      "405/405 [==============================] - 1s 1ms/step - loss: 0.7135 - acc: 0.7012\n",
      "Epoch 62/200\n",
      "405/405 [==============================] - 0s 1ms/step - loss: 0.7742 - acc: 0.6642\n",
      "Epoch 63/200\n",
      "405/405 [==============================] - 0s 984us/step - loss: 0.7485 - acc: 0.6840\n",
      "Epoch 64/200\n",
      "405/405 [==============================] - 0s 807us/step - loss: 0.6763 - acc: 0.7309\n",
      "Epoch 65/200\n",
      "405/405 [==============================] - 0s 894us/step - loss: 0.6824 - acc: 0.7160\n",
      "Epoch 66/200\n",
      "405/405 [==============================] - 1s 1ms/step - loss: 0.7383 - acc: 0.6938\n",
      "Epoch 67/200\n",
      "405/405 [==============================] - 1s 2ms/step - loss: 0.6847 - acc: 0.7111\n",
      "Epoch 68/200\n",
      "405/405 [==============================] - 0s 1ms/step - loss: 0.6410 - acc: 0.7185\n",
      "Epoch 69/200\n",
      "405/405 [==============================] - 0s 796us/step - loss: 0.6578 - acc: 0.7037\n",
      "Epoch 70/200\n",
      "405/405 [==============================] - 0s 755us/step - loss: 0.6648 - acc: 0.7333\n",
      "Epoch 71/200\n",
      "405/405 [==============================] - 0s 733us/step - loss: 0.6832 - acc: 0.7333\n",
      "Epoch 72/200\n",
      "405/405 [==============================] - 0s 792us/step - loss: 0.6488 - acc: 0.7481\n",
      "Epoch 73/200\n",
      "405/405 [==============================] - 1s 1ms/step - loss: 0.6534 - acc: 0.7309\n",
      "Epoch 74/200\n",
      "405/405 [==============================] - 0s 1ms/step - loss: 0.7371 - acc: 0.7062\n",
      "Epoch 75/200\n",
      "405/405 [==============================] - 0s 924us/step - loss: 0.7414 - acc: 0.6988\n",
      "Epoch 76/200\n",
      "405/405 [==============================] - 0s 788us/step - loss: 0.7569 - acc: 0.6741\n",
      "Epoch 77/200\n",
      "405/405 [==============================] - 0s 750us/step - loss: 0.6197 - acc: 0.7235\n",
      "Epoch 78/200\n",
      "405/405 [==============================] - 0s 744us/step - loss: 0.5997 - acc: 0.7506\n",
      "Epoch 79/200\n",
      "405/405 [==============================] - 0s 731us/step - loss: 0.5868 - acc: 0.7679\n",
      "Epoch 80/200\n",
      "405/405 [==============================] - 0s 747us/step - loss: 0.6062 - acc: 0.7556\n",
      "Epoch 81/200\n",
      "405/405 [==============================] - 0s 772us/step - loss: 0.6084 - acc: 0.7753\n",
      "Epoch 82/200\n",
      "405/405 [==============================] - 0s 783us/step - loss: 0.6198 - acc: 0.7407\n",
      "Epoch 83/200\n",
      "405/405 [==============================] - 0s 820us/step - loss: 0.5850 - acc: 0.7531\n",
      "Epoch 84/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "405/405 [==============================] - 0s 738us/step - loss: 0.5948 - acc: 0.7481\n",
      "Epoch 85/200\n",
      "405/405 [==============================] - 0s 805us/step - loss: 0.5550 - acc: 0.7630\n",
      "Epoch 86/200\n",
      "405/405 [==============================] - 0s 755us/step - loss: 0.5884 - acc: 0.7556\n",
      "Epoch 87/200\n",
      "405/405 [==============================] - 0s 788us/step - loss: 0.5767 - acc: 0.7235\n",
      "Epoch 88/200\n",
      "405/405 [==============================] - 0s 794us/step - loss: 0.5975 - acc: 0.7333\n",
      "Epoch 89/200\n",
      "405/405 [==============================] - 0s 775us/step - loss: 0.5169 - acc: 0.7901\n",
      "Epoch 90/200\n",
      "405/405 [==============================] - 0s 793us/step - loss: 0.5172 - acc: 0.7951\n",
      "Epoch 91/200\n",
      "405/405 [==============================] - 0s 728us/step - loss: 0.4852 - acc: 0.8123\n",
      "Epoch 92/200\n",
      "405/405 [==============================] - 0s 794us/step - loss: 0.4586 - acc: 0.8049\n",
      "Epoch 93/200\n",
      "405/405 [==============================] - 0s 756us/step - loss: 0.5225 - acc: 0.8049\n",
      "Epoch 94/200\n",
      "405/405 [==============================] - 0s 764us/step - loss: 0.5327 - acc: 0.7654\n",
      "Epoch 95/200\n",
      "405/405 [==============================] - 0s 760us/step - loss: 0.4227 - acc: 0.8346\n",
      "Epoch 96/200\n",
      "405/405 [==============================] - 0s 770us/step - loss: 0.4612 - acc: 0.8123\n",
      "Epoch 97/200\n",
      "405/405 [==============================] - 0s 736us/step - loss: 0.4367 - acc: 0.8247\n",
      "Epoch 98/200\n",
      "405/405 [==============================] - 0s 801us/step - loss: 0.4984 - acc: 0.8173\n",
      "Epoch 99/200\n",
      "405/405 [==============================] - 0s 727us/step - loss: 0.5228 - acc: 0.7852\n",
      "Epoch 100/200\n",
      "405/405 [==============================] - 0s 801us/step - loss: 0.5461 - acc: 0.7753\n",
      "Epoch 101/200\n",
      "405/405 [==============================] - 0s 745us/step - loss: 0.3618 - acc: 0.8444\n",
      "Epoch 102/200\n",
      "405/405 [==============================] - 0s 806us/step - loss: 0.3986 - acc: 0.8519\n",
      "Epoch 103/200\n",
      "405/405 [==============================] - 0s 755us/step - loss: 0.4072 - acc: 0.8370\n",
      "Epoch 104/200\n",
      "405/405 [==============================] - 0s 750us/step - loss: 0.4125 - acc: 0.8198\n",
      "Epoch 105/200\n",
      "405/405 [==============================] - 0s 815us/step - loss: 0.3830 - acc: 0.8667\n",
      "Epoch 106/200\n",
      "405/405 [==============================] - 0s 730us/step - loss: 0.4384 - acc: 0.8222\n",
      "Epoch 107/200\n",
      "405/405 [==============================] - 0s 758us/step - loss: 0.3493 - acc: 0.8519\n",
      "Epoch 108/200\n",
      "405/405 [==============================] - 0s 775us/step - loss: 0.4425 - acc: 0.8395\n",
      "Epoch 109/200\n",
      "405/405 [==============================] - 0s 772us/step - loss: 0.5181 - acc: 0.7926\n",
      "Epoch 110/200\n",
      "405/405 [==============================] - 0s 763us/step - loss: 0.3685 - acc: 0.8667\n",
      "Epoch 111/200\n",
      "405/405 [==============================] - 0s 807us/step - loss: 0.3347 - acc: 0.8667\n",
      "Epoch 112/200\n",
      "405/405 [==============================] - 0s 796us/step - loss: 0.5000 - acc: 0.8025\n",
      "Epoch 113/200\n",
      "405/405 [==============================] - 0s 791us/step - loss: 0.4449 - acc: 0.8173\n",
      "Epoch 114/200\n",
      "405/405 [==============================] - 0s 813us/step - loss: 0.2992 - acc: 0.9037\n",
      "Epoch 115/200\n",
      "405/405 [==============================] - 0s 797us/step - loss: 0.2972 - acc: 0.8938\n",
      "Epoch 116/200\n",
      "405/405 [==============================] - 0s 728us/step - loss: 0.4346 - acc: 0.8123\n",
      "Epoch 117/200\n",
      "405/405 [==============================] - 0s 760us/step - loss: 0.3208 - acc: 0.8815\n",
      "Epoch 118/200\n",
      "405/405 [==============================] - 0s 745us/step - loss: 0.2448 - acc: 0.9086\n",
      "Epoch 119/200\n",
      "405/405 [==============================] - 0s 722us/step - loss: 0.4970 - acc: 0.8247\n",
      "Epoch 120/200\n",
      "405/405 [==============================] - 0s 743us/step - loss: 0.5420 - acc: 0.8025\n",
      "Epoch 121/200\n",
      "405/405 [==============================] - 0s 722us/step - loss: 0.3259 - acc: 0.8716\n",
      "Epoch 122/200\n",
      "405/405 [==============================] - 0s 774us/step - loss: 0.2530 - acc: 0.8963\n",
      "Epoch 123/200\n",
      "405/405 [==============================] - 0s 729us/step - loss: 0.2434 - acc: 0.9062\n",
      "Epoch 124/200\n",
      "405/405 [==============================] - 0s 761us/step - loss: 0.5124 - acc: 0.8370\n",
      "Epoch 125/200\n",
      "405/405 [==============================] - 0s 732us/step - loss: 0.3614 - acc: 0.8469\n",
      "Epoch 126/200\n",
      "405/405 [==============================] - 0s 731us/step - loss: 0.2738 - acc: 0.8840\n",
      "Epoch 127/200\n",
      "405/405 [==============================] - 0s 738us/step - loss: 0.2661 - acc: 0.9062\n",
      "Epoch 128/200\n",
      "405/405 [==============================] - 0s 766us/step - loss: 0.2738 - acc: 0.8963\n",
      "Epoch 129/200\n",
      "405/405 [==============================] - 0s 761us/step - loss: 0.2750 - acc: 0.9062\n",
      "Epoch 130/200\n",
      "405/405 [==============================] - 0s 761us/step - loss: 0.4261 - acc: 0.8296\n",
      "Epoch 131/200\n",
      "405/405 [==============================] - 0s 747us/step - loss: 0.2646 - acc: 0.8864\n",
      "Epoch 132/200\n",
      "405/405 [==============================] - 0s 748us/step - loss: 0.2518 - acc: 0.8889\n",
      "Epoch 133/200\n",
      "405/405 [==============================] - 0s 736us/step - loss: 0.2158 - acc: 0.9309\n",
      "Epoch 134/200\n",
      "405/405 [==============================] - 0s 750us/step - loss: 0.2833 - acc: 0.8914\n",
      "Epoch 135/200\n",
      "405/405 [==============================] - 0s 739us/step - loss: 0.2514 - acc: 0.8963\n",
      "Epoch 136/200\n",
      "405/405 [==============================] - 0s 727us/step - loss: 0.2306 - acc: 0.9383\n",
      "Epoch 137/200\n",
      "405/405 [==============================] - 0s 744us/step - loss: 0.2182 - acc: 0.9062\n",
      "Epoch 138/200\n",
      "405/405 [==============================] - 0s 729us/step - loss: 0.5164 - acc: 0.7926\n",
      "Epoch 139/200\n",
      "405/405 [==============================] - 0s 732us/step - loss: 0.3803 - acc: 0.8568\n",
      "Epoch 140/200\n",
      "405/405 [==============================] - 0s 738us/step - loss: 0.2953 - acc: 0.8914\n",
      "Epoch 141/200\n",
      "405/405 [==============================] - 0s 770us/step - loss: 0.2148 - acc: 0.9160\n",
      "Epoch 142/200\n",
      "405/405 [==============================] - 0s 811us/step - loss: 0.1569 - acc: 0.9580\n",
      "Epoch 143/200\n",
      "405/405 [==============================] - 0s 798us/step - loss: 0.1717 - acc: 0.9333\n",
      "Epoch 144/200\n",
      "405/405 [==============================] - 0s 840us/step - loss: 0.1374 - acc: 0.9580\n",
      "Epoch 145/200\n",
      "405/405 [==============================] - 0s 807us/step - loss: 0.1982 - acc: 0.9136\n",
      "Epoch 146/200\n",
      "405/405 [==============================] - 0s 793us/step - loss: 0.4461 - acc: 0.8321\n",
      "Epoch 147/200\n",
      "405/405 [==============================] - 0s 773us/step - loss: 0.2892 - acc: 0.8914\n",
      "Epoch 148/200\n",
      "405/405 [==============================] - 0s 797us/step - loss: 0.6431 - acc: 0.7630\n",
      "Epoch 149/200\n",
      "405/405 [==============================] - 0s 787us/step - loss: 0.3009 - acc: 0.9062\n",
      "Epoch 150/200\n",
      "405/405 [==============================] - 0s 788us/step - loss: 0.2265 - acc: 0.9284\n",
      "Epoch 151/200\n",
      "405/405 [==============================] - 0s 766us/step - loss: 0.1716 - acc: 0.9432\n",
      "Epoch 152/200\n",
      "405/405 [==============================] - 0s 730us/step - loss: 0.1539 - acc: 0.9432\n",
      "Epoch 153/200\n",
      "405/405 [==============================] - 0s 765us/step - loss: 0.1222 - acc: 0.9531\n",
      "Epoch 154/200\n",
      "405/405 [==============================] - 0s 767us/step - loss: 0.1263 - acc: 0.9605\n",
      "Epoch 155/200\n",
      "405/405 [==============================] - 0s 954us/step - loss: 0.3123 - acc: 0.8963\n",
      "Epoch 156/200\n",
      "405/405 [==============================] - 0s 805us/step - loss: 0.2628 - acc: 0.8963\n",
      "Epoch 157/200\n",
      "405/405 [==============================] - 0s 758us/step - loss: 0.1816 - acc: 0.9432\n",
      "Epoch 158/200\n",
      "405/405 [==============================] - 0s 721us/step - loss: 0.1020 - acc: 0.9704\n",
      "Epoch 159/200\n",
      "405/405 [==============================] - 0s 765us/step - loss: 0.1768 - acc: 0.9481\n",
      "Epoch 160/200\n",
      "405/405 [==============================] - 0s 802us/step - loss: 0.3175 - acc: 0.8765\n",
      "Epoch 161/200\n",
      "405/405 [==============================] - 0s 749us/step - loss: 0.1368 - acc: 0.9531\n",
      "Epoch 162/200\n",
      "405/405 [==============================] - 0s 728us/step - loss: 0.1075 - acc: 0.9679\n",
      "Epoch 163/200\n",
      "405/405 [==============================] - 0s 775us/step - loss: 0.1362 - acc: 0.9531\n",
      "Epoch 164/200\n",
      "405/405 [==============================] - 0s 728us/step - loss: 0.1191 - acc: 0.9556\n",
      "Epoch 165/200\n",
      "405/405 [==============================] - 0s 729us/step - loss: 0.3019 - acc: 0.8840\n",
      "Epoch 166/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "405/405 [==============================] - 0s 745us/step - loss: 0.5095 - acc: 0.8593\n",
      "Epoch 167/200\n",
      "405/405 [==============================] - 0s 727us/step - loss: 0.2162 - acc: 0.9210\n",
      "Epoch 168/200\n",
      "405/405 [==============================] - 0s 721us/step - loss: 0.2752 - acc: 0.8889\n",
      "Epoch 169/200\n",
      "405/405 [==============================] - 0s 736us/step - loss: 0.2951 - acc: 0.8914\n",
      "Epoch 170/200\n",
      "405/405 [==============================] - 0s 742us/step - loss: 0.1297 - acc: 0.9605\n",
      "Epoch 171/200\n",
      "405/405 [==============================] - 0s 721us/step - loss: 0.0929 - acc: 0.9605\n",
      "Epoch 172/200\n",
      "405/405 [==============================] - 0s 729us/step - loss: 0.0648 - acc: 0.9827\n",
      "Epoch 173/200\n",
      "405/405 [==============================] - 0s 720us/step - loss: 0.1172 - acc: 0.9580\n",
      "Epoch 174/200\n",
      "405/405 [==============================] - 0s 727us/step - loss: 0.2044 - acc: 0.9383\n",
      "Epoch 175/200\n",
      "405/405 [==============================] - 0s 705us/step - loss: 0.2385 - acc: 0.9136\n",
      "Epoch 176/200\n",
      "405/405 [==============================] - 0s 723us/step - loss: 0.1249 - acc: 0.9556\n",
      "Epoch 177/200\n",
      "405/405 [==============================] - 0s 706us/step - loss: 0.1361 - acc: 0.9556\n",
      "Epoch 178/200\n",
      "405/405 [==============================] - 0s 706us/step - loss: 0.0717 - acc: 0.9778\n",
      "Epoch 179/200\n",
      "405/405 [==============================] - 0s 704us/step - loss: 0.0976 - acc: 0.9605\n",
      "Epoch 180/200\n",
      "405/405 [==============================] - 0s 715us/step - loss: 0.1876 - acc: 0.9358\n",
      "Epoch 181/200\n",
      "405/405 [==============================] - 0s 795us/step - loss: 0.1889 - acc: 0.9210\n",
      "Epoch 182/200\n",
      "405/405 [==============================] - 0s 788us/step - loss: 0.3947 - acc: 0.8741\n",
      "Epoch 183/200\n",
      "405/405 [==============================] - 0s 725us/step - loss: 0.3209 - acc: 0.8938\n",
      "Epoch 184/200\n",
      "405/405 [==============================] - 0s 782us/step - loss: 0.1460 - acc: 0.9531\n",
      "Epoch 185/200\n",
      "405/405 [==============================] - 0s 757us/step - loss: 0.0649 - acc: 0.9852\n",
      "Epoch 186/200\n",
      "405/405 [==============================] - 0s 726us/step - loss: 0.0626 - acc: 0.9802\n",
      "Epoch 187/200\n",
      "405/405 [==============================] - 0s 764us/step - loss: 0.0592 - acc: 0.9802\n",
      "Epoch 188/200\n",
      "405/405 [==============================] - 0s 810us/step - loss: 0.1276 - acc: 0.9531\n",
      "Epoch 189/200\n",
      "405/405 [==============================] - 0s 729us/step - loss: 0.1425 - acc: 0.9481\n",
      "Epoch 190/200\n",
      "405/405 [==============================] - 0s 753us/step - loss: 0.1682 - acc: 0.9556\n",
      "Epoch 191/200\n",
      "405/405 [==============================] - 0s 740us/step - loss: 0.0599 - acc: 0.9827\n",
      "Epoch 192/200\n",
      "405/405 [==============================] - 0s 731us/step - loss: 0.0454 - acc: 0.9877\n",
      "Epoch 193/200\n",
      "405/405 [==============================] - 0s 811us/step - loss: 0.0362 - acc: 0.9926\n",
      "Epoch 194/200\n",
      "405/405 [==============================] - 0s 798us/step - loss: 0.0319 - acc: 0.9926\n",
      "Epoch 195/200\n",
      "405/405 [==============================] - 0s 833us/step - loss: 0.0306 - acc: 0.9926\n",
      "Epoch 196/200\n",
      "405/405 [==============================] - 0s 761us/step - loss: 0.0347 - acc: 0.9901\n",
      "Epoch 197/200\n",
      "405/405 [==============================] - 0s 760us/step - loss: 0.0264 - acc: 0.9926\n",
      "Epoch 198/200\n",
      "405/405 [==============================] - 0s 754us/step - loss: 0.0263 - acc: 0.9926\n",
      "Epoch 199/200\n",
      "405/405 [==============================] - 0s 740us/step - loss: 0.0240 - acc: 0.9926\n",
      "Epoch 200/200\n",
      "405/405 [==============================] - 0s 790us/step - loss: 0.0215 - acc: 0.9926\n",
      "174/174 [==============================] - 0s 761us/step\n",
      "Model performance:\n",
      "acc,43.103%\n"
     ]
    }
   ],
   "source": [
    "# Keras model\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "Xdf = pd.read_csv('x_matrix.csv', sep=',', dtype='float', index_col=0)\n",
    "ydf = pd.read_csv('y_vec.csv', sep=',', dtype='int32', index_col=0)\n",
    "\n",
    "Xdf = Xdf.values\n",
    "ydf = np.array(ydf).ravel()\n",
    "\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = ydf.reshape(len(ydf), 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "\n",
    "# split data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xdf, onehot_encoded, test_size=0.3, random_state=20)\n",
    "input_dim = len(X_train[0])\n",
    "\n",
    "# find number of categories\n",
    "num_categories = max(ydf)+1\n",
    "\n",
    "model_nn = Sequential()\n",
    "model_nn.add(Dense(input_dim, input_dim=input_dim, activation='relu'))\n",
    "model_nn.add(Dense(50, activation='relu'))\n",
    "model_nn.add(Dense(50, activation='relu'))\n",
    "model_nn.add(Dense(input_dim, activation='relu'))\n",
    "model_nn.add(Dense(num_categories, activation='softmax'))\n",
    "\n",
    "model_nn.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model_nn.fit(X_train, y_train, epochs=200, batch_size=10, verbose=1)\n",
    "scores = model_nn.evaluate(x=X_test,y=y_test)\n",
    "print(\"Model performance:\")\n",
    "print(\"%s,%.3f%%\" % (model_nn.metrics_names[1], scores[1]*100))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model_nn.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
